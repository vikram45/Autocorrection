{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20f1885f",
   "metadata": {},
   "source": [
    "# Autocorrection usinng Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0b256a",
   "metadata": {},
   "source": [
    "With the context of machine learning, autocorrect is based on natural language processing. As the name suggests it is programmed to correct spellings and errors while typing. So how it works?\n",
    "\n",
    "\n",
    "Before I get into the coding stuff let’s understand how autocorrect works. Let’s say you typed a word in your keyboard if the word will exist in the vocabulary of our smartphone then it will assume that you have written the right word. Now it doesn’t matter whether you write a name, a noun or any word on the planet.\n",
    "\n",
    "If the word exists in the history of the smartphone, it will generalize the word as a correct word. What if the word doesn’t exist? If the word that you typed is a non-existing word in the history of our smartphone then the autocorrect is programmed to find the most similar words in the history of our smartphone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fb9c01",
   "metadata": {},
   "source": [
    "For this task, we need some libraries. The libraries that I am going to use are very general as a machine learning practitioner. So you must be having all the libraries installed in your system already except one. You need to install a library known as textdistance, which can be easily installed by using the pip command; pip install textdistance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a69c3bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first ten words in the text are: \n",
      "['the', 'project', 'gutenberg', 'ebook', 'of', 'moby', 'dick', 'or', 'the', 'whale']\n",
      "There are 17647 unique words in the vocabulary.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import textdistance\n",
    "import re\n",
    "from collections import Counter\n",
    "words = []\n",
    "with open('book.txt', 'r',encoding=\"utf8\") as f:\n",
    "    file_name_data = f.read()\n",
    "    file_name_data=file_name_data.lower()\n",
    "    words = re.findall('\\w+',file_name_data)\n",
    "# This is our vocabulary\n",
    "V = set(words)\n",
    "print(f\"The first ten words in the text are: \\n{words[0:10]}\")\n",
    "print(f\"There are {len(V)} unique words in the vocabulary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf0aa32",
   "metadata": {},
   "source": [
    "In the above code, we made a list of words, and now we need to build the frequency of those words, which can be easily done by using the counter function in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfb1ca67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 14703), ('of', 6742), ('and', 6517), ('a', 4799), ('to', 4707), ('in', 4238), ('that', 3081), ('it', 2534), ('his', 2530), ('i', 2120)]\n"
     ]
    }
   ],
   "source": [
    "word_freq_dict = {}  \n",
    "word_freq_dict = Counter(words)\n",
    "print(word_freq_dict.most_common()[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ba49e7",
   "metadata": {},
   "source": [
    "### Relative Frequency of words\n",
    "Now we want to get the probability of occurrence of each word, this equals the relative frequencies of the words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a67d18f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = {}     \n",
    "Total = sum(word_freq_dict.values())    \n",
    "for k in word_freq_dict.keys():\n",
    "    probs[k] = word_freq_dict[k]/Total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6acd83",
   "metadata": {},
   "source": [
    "### Finding Similar Words\n",
    "Now we will sort similar words according to the Jaccard distance by calculating the 2 grams Q of the words. Next, we will return the 5 most similar words ordered by similarity and probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2733ac31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_autocorrect(input_word):\n",
    "    input_word = input_word.lower()\n",
    "    if input_word in V:\n",
    "        return('Your word seems to be correct')\n",
    "    else:\n",
    "        similarities = [1-(textdistance.Jaccard(qval=2).distance(v,input_word)) for v in word_freq_dict.keys()]\n",
    "        df = pd.DataFrame.from_dict(probs, orient='index').reset_index()\n",
    "        df = df.rename(columns={'index':'Word', 0:'Prob'})\n",
    "        df['Similarity'] = similarities\n",
    "        output = df.sort_values(['Similarity', 'Prob'], ascending=False).head()\n",
    "        return(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4f2ec07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Prob</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2571</th>\n",
       "      <td>nevertheless</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13657</th>\n",
       "      <td>boneless</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12684</th>\n",
       "      <td>elevates</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1105</th>\n",
       "      <td>never</td>\n",
       "      <td>0.000925</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7136</th>\n",
       "      <td>level</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Word      Prob  Similarity\n",
       "2571   nevertheless  0.000225    0.750000\n",
       "13657      boneless  0.000013    0.416667\n",
       "12684      elevates  0.000004    0.416667\n",
       "1105          never  0.000925    0.400000\n",
       "7136          level  0.000108    0.400000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_autocorrect('neverteless')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad723d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bee54da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
